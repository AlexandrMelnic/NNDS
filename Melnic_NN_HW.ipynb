{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di NNDS_Homework_Template.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wAEgygyPfO7b"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdnnRxTNdyW1"
      },
      "source": [
        "# Neural Networks for Data Science Applications\n",
        "## Mid-term Homework: Implementing a custom activation function\n",
        "\n",
        "**Name**: *\\<Alexandru Melnic\\>*\n",
        "\n",
        "**Matricola**: *\\<1692625\\>*\n",
        "\n",
        "Send the completed notebook before 26/11/2020 back to **simone.scardapane@uniroma1.it** with the object \"[NNDS] Homework_1_\\<id\\>\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEr8qV6-nMuL"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAEgygyPfO7b"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The **exponential linear unit** (ELU) is an activation function defined as [1]:\n",
        "\n",
        "$$\n",
        "\\phi(x) =\n",
        "\\Biggl\\{ \n",
        "\\begin{align} \n",
        "x & \\;\\; \\text{ if } x \\ge 0 \\\\\n",
        "\\alpha \\left(\\exp\\left(x\\right)- 1\\right) & \\;\\; \\text{ otherwise } \n",
        "\\end{align}\n",
        "\\Bigr.\n",
        "\\,,\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is a hyper-parameter. The function is implemented in `tf.keras.layers.ELU` (see the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU)).\n",
        "\n",
        "The **parametric ELU** (PELU) extends the ELU activation function as [2]:\n",
        "\n",
        "$$\n",
        "\\phi(x) =\n",
        "\\Biggl\\{ \n",
        "\\begin{align} \n",
        "\\frac{\\alpha}{\\beta}x & \\;\\; \\text{ if } x \\ge 0 \\\\\n",
        "\\alpha \\left(\\exp\\Bigl(\\frac{x}{\\beta}\\Bigr)- 1\\right) & \\;\\; \\text{ otherwise } \n",
        "\\end{align}\n",
        "\\Bigr.\n",
        "\\,,\n",
        "$$\n",
        "\n",
        "where the major difference is that $\\alpha,\\beta > 0$ are *trainable* parameters, i.e., a pair of $(\\alpha, \\beta)$ values is trained for each unit in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u4aF6Z4maHd"
      },
      "source": [
        "### Exercise 1: implement the PELU\n",
        "\n",
        "In TensorFlow, it is possible to implement new layers by subclassing `tf.keras.layers.Layer`:\n",
        "\n",
        "+ [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
        "+ [Custom layers](https://www.tensorflow.org/tutorials/customization/custom_layers)\n",
        "+ [tf.keras.layers.Layer (documentation)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)\n",
        "\n",
        "**Exercise 1**: *After carefully reading the guides*, complete the following implementation of the PELU activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfCpFg2xdug_"
      },
      "source": [
        "from tensorflow.keras.constraints import NonNeg\n",
        "\n",
        "class PELU(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units=32):\n",
        "        super(PELU, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape=_):\n",
        "\n",
        "        param_init = tf.random_uniform_initializer(minval=0.1, maxval=1)\n",
        "\n",
        "        self.params = self.add_weight(\n",
        "            shape = (self.units, 2), \n",
        "            initializer = param_init, \n",
        "            trainable = True, \n",
        "            constraint = NonNeg(), \n",
        "            dtype='float32'\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        a, b = self.params[:,0], self.params[:,1]\n",
        "        return a/b*inputs*tf.cast(inputs>=0, dtype='float32') + a*(tf.exp(inputs/b*tf.cast(inputs<0, dtype='float32'))-1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8MEZxqbn8vs"
      },
      "source": [
        "**Hints for a correct implementation**:\n",
        "\n",
        "1. The layer (probably) requires two sets of trainable variables, whose shape depends on the number of units.\n",
        "2. From the definition of the PELU, $\\alpha, \\beta$ are required to be positive in order to ensure differentiability. The simplest way to handle this is to use a [constraint callable](https://www.tensorflow.org/api_docs/python/tf/keras/constraints) when creating the weight (see also the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) for `add_weight`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QY0TfCxo9yd"
      },
      "source": [
        "### Exercise 2: some preliminary tests\n",
        "\n",
        "To evaluate your implementation, let us start by creating a single PELU function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWY76mEepSgj"
      },
      "source": [
        "**Exercise 2.1**: plot the function using the skeleton code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT7gvDK44fHI"
      },
      "source": [
        "pelu = PELU(units=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdK0CyscfDtC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "e74f81bd-42f1-433e-d427-616e363d0b0f"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_range = tf.linspace(-5, 5, 200) # An equispaced grid of 200 points in [-5, +5]\n",
        "\n",
        "y_range = pelu(x_range)\n",
        "\n",
        "plt.plot(x_range.numpy(), y_range.numpy())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer pelu is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa3b0520668>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyGIlR_aqayc"
      },
      "source": [
        "The derivative of a PELU function with respect to the $\\alpha$ parameter is given by [2]:\n",
        "\n",
        "$$\n",
        "\\frac{d\\phi(x)}{d\\alpha} =\n",
        "\\Biggl\\{ \n",
        "\\begin{align} \n",
        "\\frac{x}{\\beta} & \\;\\; \\text{ if } x \\ge 0 \\\\\n",
        " \\left(\\exp\\Bigl(\\frac{x}{\\beta}\\Bigr)- 1\\right) & \\;\\; \\text{ otherwise } \n",
        "\\end{align}\n",
        "\\Bigr.\n",
        "\\,,\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnvkoRVAqwg1"
      },
      "source": [
        "\n",
        "**Exercise 2.2**: using a `tf.GradientTape` object, compute the derivative above using automatic differentiation, and check its correctness up to a certain numerical precision.\n",
        "\n",
        "**Hints for a correct implementation**:\n",
        "\n",
        "1. `tf.GradientTape` allows to compute the derivative *at a single point x*. If you prefer to avoid a loop over all possible points, consider using the `jacobian` function to obtain them in a single pass ([Advanced Automatic Differentiation](https://www.tensorflow.org/guide/advanced_autodiff)).\n",
        "2. Given two tensors x and y, a simple way to compute elementwise similarity up to a certain precision (e.g., $10^{-4}$), is given by `tf.reduce_all(tf.abs(x - y) < 1e-4)`.\n",
        "\n",
        "**Exercise 2.3 (optional)**: try the same for the $\\beta$ parameter (you can check the analytical formula for the gradient in the original paper [2]). **Careful**: the equation in the original paper has a missing $h$ (thanks to Davide Aureli and Federico Siciliano for spotting this). See [the correct derivation](https://www.wolframalpha.com/input/?i=d%28a*%28exp%28h%2Fb%29-1%29%29%2Fdb) on Wolfram Alpha."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO_vjCl7w_DK"
      },
      "source": [
        "# Analytical gradient\n",
        "def pelu_gradient(x, params):\n",
        "    a, b = params[:,0], params[:,1]\n",
        "    \n",
        "    grad_a = tf.where(x>=0, x/b, tf.exp(x/b)-1)\n",
        "    grad_b = tf.where(x>=0, -a/b**2*x, -a*x/b**2*tf.exp(x/b))\n",
        "    \n",
        "    return tf.stack([grad_a, grad_b], axis=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV3gWEPkslkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee4c1da-4a41-456a-bdf3-5ee66f22c6ec"
      },
      "source": [
        "# tf gradient\n",
        "\n",
        "pelu = PELU(units=1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    \n",
        "    pelu_fun = pelu(x_range)\n",
        "\n",
        "grad = tape.jacobian(pelu_fun, pelu.params)\n",
        "\n",
        "tf_gradient = tf.reshape(grad, shape=(-1, 2))\n",
        "my_gradient = pelu_gradient(tf.cast(x_range, tf.float32), pelu.params)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer pelu_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bp59PXeW7WV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e39270-5df7-4af3-a8f1-6e86ad82b806"
      },
      "source": [
        "# check the difference between the two\n",
        "\n",
        "tf.reduce_all(tf.abs(tf_gradient - my_gradient) < 1e-4)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yjnVl6Ysm7F"
      },
      "source": [
        "### Exercise 3: PELU in practice\n",
        "\n",
        "Consider a simple model built with the PELU activation function, as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kET2bH2QR4ux"
      },
      "source": [
        "model = tf.keras.Sequential(layers=[\n",
        "      tf.keras.layers.Dense(50),\n",
        "      PELU(50),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MgOOP5T44dI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "train_set = pd.read_csv('/content/sample_data/mnist_train_small.csv', header=None)\n",
        "X = train_set.iloc[:, 1:].values.astype('float32')\n",
        "y = train_set.iloc[:, 0].values.reshape(-1,1).astype('float32')\n",
        "del train_set\n",
        "\n",
        "test_set = pd.read_csv('/content/sample_data/mnist_test.csv', header=None)\n",
        "X_test = test_set.iloc[:, 1:].values.astype('float32')\n",
        "y_test = test_set.iloc[:, 0].values.reshape(-1,1).astype('float32')\n",
        "del test_set\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "\n",
        "train_set = tf.data.Dataset.from_tensor_slices((scaler.transform(X), y))\n",
        "train_set = train_set.shuffle(buffer_size = 1000).batch(BATCH_SIZE)\n",
        "\n",
        "test_set = tf.data.Dataset.from_tensor_slices((scaler.transform(X_test), y_test)).batch(BATCH_SIZE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYNqos6o-BYP",
        "outputId": "8f38be43-e94a-4b66-995f-3cf7e8d9312f"
      },
      "source": [
        "NUM_EPOCH = 10\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(1e-3), \n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "            metrics='accuracy')\n",
        "\n",
        "model.fit(train_set, epochs=NUM_EPOCH)\n",
        "model.evaluate(test_set)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4755 - accuracy: 0.5688\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.7294 - accuracy: 0.7983\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.5662 - accuracy: 0.8415\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4832 - accuracy: 0.8637\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4304 - accuracy: 0.8781\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3940 - accuracy: 0.8867\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3661 - accuracy: 0.8935\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3439 - accuracy: 0.8994\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3255 - accuracy: 0.9048\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3100 - accuracy: 0.9091\n",
            "Test loss and accuracy\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3448 - accuracy: 0.9059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.34478333592414856, 0.9059000015258789]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvVbeTDwtdbu"
      },
      "source": [
        "**Exercise 3**: load any classification dataset, and train the model above (using either a custom training loop or `model.fit(...)`). Additionally, compare with a standard ReLU activation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0fkDrIJ_f24"
      },
      "source": [
        "In the next cells there will be the confront between the model with the ReLU and the one with the PELU. The two models will have the same hyperparameters: 10 epochs, SGD optimizer with lr 10**-3. The confront will be on the fitting time, evaluation time,  the train loss and accuracy and the test loss and accuracy. All the experiment will be repeated 20 times and at the end the averages of all the measurements will be shown.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD86hYDoFn9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "1138e940-6f40-476a-93bb-8987ca3758cf"
      },
      "source": [
        "#@title Code needed to confront the pelu and relu models\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "iterations = 20\n",
        "NUM_EPOCH = 10\n",
        "\n",
        "tot_stats_pelu = []\n",
        "tot_stats_relu = []\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "\n",
        "    current_stats = []\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = tf.keras.Sequential(layers=[\n",
        "        tf.keras.layers.Dense(50),\n",
        "        PELU(50),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "                  optimizer = tf.keras.optimizers.SGD(10**-3), \n",
        "                  metrics = 'accuracy')\n",
        "    \n",
        "    start_time = time() # for training time\n",
        "    history_pelu = model.fit(train_set, \n",
        "                             epochs = NUM_EPOCH, \n",
        "                             callbacks=[tf.keras.callbacks.History()],\n",
        "                             verbose = 0)\n",
        "    current_stats.append(time() - start_time)\n",
        "\n",
        "    start_time = time() # for evaluation time\n",
        "    test_results_pelu = model.evaluate(test_set, verbose=0)\n",
        "    current_stats.append(time() - start_time)\n",
        "\n",
        "    current_stats.append(history_pelu.history['loss'][-1])\n",
        "    current_stats.append(history_pelu.history['accuracy'][-1])\n",
        "    current_stats.append(test_results_pelu[0])\n",
        "    current_stats.append(test_results_pelu[1])\n",
        "    tot_stats_pelu.append(current_stats)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    current_stats = []\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = tf.keras.Sequential(layers=[\n",
        "        tf.keras.layers.Dense(50, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "                  optimizer = tf.keras.optimizers.SGD(10**-3), \n",
        "                  metrics = 'accuracy')\n",
        "    \n",
        "    start_time = time() # for training time\n",
        "    history_relu = model.fit(train_set, \n",
        "                             epochs = NUM_EPOCH,\n",
        "                             callbacks=[tf.keras.callbacks.History()],\n",
        "                             verbose = 0)\n",
        "    current_stats.append(time() - start_time)\n",
        "\n",
        "    start_time = time() # for evaluation time\n",
        "    test_results_relu = model.evaluate(test_set, verbose=0)\n",
        "    current_stats.append(time() - start_time)\n",
        "\n",
        "    current_stats.append(history_relu.history['loss'][-1])\n",
        "    current_stats.append(history_relu.history['accuracy'][-1])\n",
        "    current_stats.append(test_results_relu[0])\n",
        "    current_stats.append(test_results_relu[1])\n",
        "    tot_stats_relu.append(current_stats)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [06:44<00:00, 20.24s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzur20NeumKO"
      },
      "source": [
        "# save results in a dataframe\n",
        "\n",
        "pelu_results = pd.DataFrame(tot_stats_pelu, columns = [ 'fit_time','eval_time' ,'loss', 'acc', 'test_loss', 'test_acc'])\n",
        "relu_results = pd.DataFrame(tot_stats_relu, columns = [ 'fit_time','eval_time' ,'loss', 'acc', 'test_loss', 'test_acc'])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TH_tY4C_LIU"
      },
      "source": [
        "Average measurements for the model with ReLU after 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1zH9OWFluBC",
        "outputId": "8a21f797-b03a-4eaa-9bed-d4010ccecd7c"
      },
      "source": [
        "relu_results.mean()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fit_time     8.551721\n",
              "eval_time    0.431712\n",
              "loss         0.381503\n",
              "acc          0.894495\n",
              "test_loss    0.400977\n",
              "test_acc     0.893500\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYg8_xAQ_Tr5"
      },
      "source": [
        "Average measurements for the model with PELU after 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hocmxHT35D17",
        "outputId": "259c2139-9093-4c5b-de09-d19f84ecb9d6"
      },
      "source": [
        "pelu_results.mean()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fit_time     10.740577\n",
              "eval_time     0.483712\n",
              "loss          0.315165\n",
              "acc           0.907705\n",
              "test_loss     0.379020\n",
              "test_acc      0.900610\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgSxs2Q9ASpz"
      },
      "source": [
        "From the results above it is possible to notice that in average the training time for 10 epochs is lower in the ReLU model, that is reasonable since the computation for the PELU is higher and every feauture has a custom activation function. Lastly, the accuracy and loss on the test set of the PELU is higher than the ReLU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkb3cj9r7uUH"
      },
      "source": [
        "### Optional: understanding saving/loading of models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEjaJQT06jh9"
      },
      "source": [
        "TensorFlow has several options for saving or loading objects from the disk:\n",
        "\n",
        "1. [Save and load Keras models](https://www.tensorflow.org/guide/keras/save_and_serialize/)\n",
        "\n",
        "In many cases, custom classes require the implementation of a `get_config` / `from_config` functions to define the serialization behaviour.\n",
        "\n",
        "**Exercise 4 (optional)**: implement the `get_config` method and test your implementation as below (taken from the guide on saving and loading models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh70mgivF3kP"
      },
      "source": [
        "model = tf.keras.Sequential(layers=[\n",
        "      tf.keras.layers.Dense(50),\n",
        "      PELU(50),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voxx7kntvM0W"
      },
      "source": [
        "model.save('pelu_model')\n",
        "del PELU # This is needed to remove any reference to PELU from memory\n",
        "reloaded_model = tf.keras.models.load_model('pelu_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "838PN8xN0rIW"
      },
      "source": [
        "print(\"Original model:\", model)\n",
        "print(\"Loaded model:\", reloaded_model) # Observe that the object has been dynamically recreated in absence of the configuration options"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl9dZ3HambSz"
      },
      "source": [
        "### References\n",
        "\n",
        "[1] Clevert, D.A., Unterthiner, T. and Hochreiter, S., 2015. [Fast and accurate deep network learning by exponential linear units (ELUs)](https://arxiv.org/abs/1511.07289). arXiv preprint arXiv:1511.07289.\n",
        "\n",
        "[2] Trottier, L., Gigu, P. and Chaib-draa, B., 2017. [Parametric exponential linear unit for deep convolutional neural networks](https://arxiv.org/abs/1605.09332). In 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA) (pp. 207-214). IEEE."
      ]
    }
  ]
}